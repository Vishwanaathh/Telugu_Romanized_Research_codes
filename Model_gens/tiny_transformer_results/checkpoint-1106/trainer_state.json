{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 1106,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.018083182640144666,
      "grad_norm": 4.207064628601074,
      "learning_rate": 1.98372513562387e-05,
      "loss": 0.7038320541381836,
      "step": 10
    },
    {
      "epoch": 0.03616636528028933,
      "grad_norm": 4.690009593963623,
      "learning_rate": 1.9656419529837254e-05,
      "loss": 0.692609453201294,
      "step": 20
    },
    {
      "epoch": 0.054249547920433995,
      "grad_norm": 3.4036903381347656,
      "learning_rate": 1.9475587703435806e-05,
      "loss": 0.6895005226135253,
      "step": 30
    },
    {
      "epoch": 0.07233273056057866,
      "grad_norm": 6.690968036651611,
      "learning_rate": 1.929475587703436e-05,
      "loss": 0.6738694190979004,
      "step": 40
    },
    {
      "epoch": 0.09041591320072333,
      "grad_norm": 6.010591983795166,
      "learning_rate": 1.911392405063291e-05,
      "loss": 0.6701160907745362,
      "step": 50
    },
    {
      "epoch": 0.10849909584086799,
      "grad_norm": 2.447598695755005,
      "learning_rate": 1.8933092224231466e-05,
      "loss": 0.6921838760375977,
      "step": 60
    },
    {
      "epoch": 0.12658227848101267,
      "grad_norm": 3.8624649047851562,
      "learning_rate": 1.875226039783002e-05,
      "loss": 0.6950963973999024,
      "step": 70
    },
    {
      "epoch": 0.14466546112115733,
      "grad_norm": 4.832038879394531,
      "learning_rate": 1.8571428571428575e-05,
      "loss": 0.7153438091278076,
      "step": 80
    },
    {
      "epoch": 0.162748643761302,
      "grad_norm": 2.485089063644409,
      "learning_rate": 1.8390596745027126e-05,
      "loss": 0.6730682849884033,
      "step": 90
    },
    {
      "epoch": 0.18083182640144665,
      "grad_norm": 2.530341386795044,
      "learning_rate": 1.8209764918625678e-05,
      "loss": 0.7056440830230712,
      "step": 100
    },
    {
      "epoch": 0.19891500904159132,
      "grad_norm": 6.224339485168457,
      "learning_rate": 1.8028933092224232e-05,
      "loss": 0.6659965991973877,
      "step": 110
    },
    {
      "epoch": 0.21699819168173598,
      "grad_norm": 3.147644281387329,
      "learning_rate": 1.7848101265822787e-05,
      "loss": 0.657301378250122,
      "step": 120
    },
    {
      "epoch": 0.23508137432188064,
      "grad_norm": 2.7148091793060303,
      "learning_rate": 1.7667269439421338e-05,
      "loss": 0.6943887233734131,
      "step": 130
    },
    {
      "epoch": 0.25316455696202533,
      "grad_norm": 5.2769012451171875,
      "learning_rate": 1.7486437613019893e-05,
      "loss": 0.6870117664337159,
      "step": 140
    },
    {
      "epoch": 0.27124773960216997,
      "grad_norm": 2.7742466926574707,
      "learning_rate": 1.7305605786618447e-05,
      "loss": 0.6774771690368653,
      "step": 150
    },
    {
      "epoch": 0.28933092224231466,
      "grad_norm": 2.953896999359131,
      "learning_rate": 1.7124773960217002e-05,
      "loss": 0.6515581130981445,
      "step": 160
    },
    {
      "epoch": 0.3074141048824593,
      "grad_norm": 6.272484302520752,
      "learning_rate": 1.6943942133815553e-05,
      "loss": 0.6871731758117676,
      "step": 170
    },
    {
      "epoch": 0.325497287522604,
      "grad_norm": 3.2082748413085938,
      "learning_rate": 1.6763110307414104e-05,
      "loss": 0.6931763648986816,
      "step": 180
    },
    {
      "epoch": 0.3435804701627486,
      "grad_norm": 3.4691274166107178,
      "learning_rate": 1.658227848101266e-05,
      "loss": 0.6553941249847413,
      "step": 190
    },
    {
      "epoch": 0.3616636528028933,
      "grad_norm": 5.363173484802246,
      "learning_rate": 1.6401446654611213e-05,
      "loss": 0.7039049148559571,
      "step": 200
    },
    {
      "epoch": 0.379746835443038,
      "grad_norm": 2.5617072582244873,
      "learning_rate": 1.6220614828209768e-05,
      "loss": 0.6397264957427978,
      "step": 210
    },
    {
      "epoch": 0.39783001808318263,
      "grad_norm": 2.968327283859253,
      "learning_rate": 1.603978300180832e-05,
      "loss": 0.6567267894744873,
      "step": 220
    },
    {
      "epoch": 0.4159132007233273,
      "grad_norm": 3.3370397090911865,
      "learning_rate": 1.5858951175406874e-05,
      "loss": 0.6555732727050781,
      "step": 230
    },
    {
      "epoch": 0.43399638336347196,
      "grad_norm": 2.7748851776123047,
      "learning_rate": 1.5678119349005428e-05,
      "loss": 0.6083516597747802,
      "step": 240
    },
    {
      "epoch": 0.45207956600361665,
      "grad_norm": 2.9879355430603027,
      "learning_rate": 1.549728752260398e-05,
      "loss": 0.6426461696624756,
      "step": 250
    },
    {
      "epoch": 0.4701627486437613,
      "grad_norm": 2.377814769744873,
      "learning_rate": 1.531645569620253e-05,
      "loss": 0.646650743484497,
      "step": 260
    },
    {
      "epoch": 0.488245931283906,
      "grad_norm": 5.447944641113281,
      "learning_rate": 1.5135623869801085e-05,
      "loss": 0.6827661514282226,
      "step": 270
    },
    {
      "epoch": 0.5063291139240507,
      "grad_norm": 2.8371376991271973,
      "learning_rate": 1.495479204339964e-05,
      "loss": 0.6618247509002686,
      "step": 280
    },
    {
      "epoch": 0.5244122965641953,
      "grad_norm": 5.7610273361206055,
      "learning_rate": 1.4773960216998193e-05,
      "loss": 0.730298662185669,
      "step": 290
    },
    {
      "epoch": 0.5424954792043399,
      "grad_norm": 3.4035112857818604,
      "learning_rate": 1.4593128390596747e-05,
      "loss": 0.6781655311584472,
      "step": 300
    },
    {
      "epoch": 0.5605786618444847,
      "grad_norm": 2.777360677719116,
      "learning_rate": 1.44122965641953e-05,
      "loss": 0.6622468948364257,
      "step": 310
    },
    {
      "epoch": 0.5786618444846293,
      "grad_norm": 2.8853609561920166,
      "learning_rate": 1.4231464737793851e-05,
      "loss": 0.6974113464355469,
      "step": 320
    },
    {
      "epoch": 0.596745027124774,
      "grad_norm": 5.351035118103027,
      "learning_rate": 1.4050632911392406e-05,
      "loss": 0.669132137298584,
      "step": 330
    },
    {
      "epoch": 0.6148282097649186,
      "grad_norm": 5.729823589324951,
      "learning_rate": 1.3869801084990959e-05,
      "loss": 0.6258926391601562,
      "step": 340
    },
    {
      "epoch": 0.6329113924050633,
      "grad_norm": 2.727193593978882,
      "learning_rate": 1.3688969258589514e-05,
      "loss": 0.5977882862091064,
      "step": 350
    },
    {
      "epoch": 0.650994575045208,
      "grad_norm": 3.38615345954895,
      "learning_rate": 1.3508137432188066e-05,
      "loss": 0.6304559707641602,
      "step": 360
    },
    {
      "epoch": 0.6690777576853526,
      "grad_norm": 3.4654924869537354,
      "learning_rate": 1.332730560578662e-05,
      "loss": 0.7009489059448242,
      "step": 370
    },
    {
      "epoch": 0.6871609403254972,
      "grad_norm": 2.952023983001709,
      "learning_rate": 1.3146473779385174e-05,
      "loss": 0.7248681068420411,
      "step": 380
    },
    {
      "epoch": 0.705244122965642,
      "grad_norm": 6.402263164520264,
      "learning_rate": 1.2965641952983725e-05,
      "loss": 0.6615688323974609,
      "step": 390
    },
    {
      "epoch": 0.7233273056057866,
      "grad_norm": 3.512462854385376,
      "learning_rate": 1.2784810126582278e-05,
      "loss": 0.6713387966156006,
      "step": 400
    },
    {
      "epoch": 0.7414104882459313,
      "grad_norm": 5.277689456939697,
      "learning_rate": 1.2603978300180833e-05,
      "loss": 0.6121244907379151,
      "step": 410
    },
    {
      "epoch": 0.759493670886076,
      "grad_norm": 5.59175968170166,
      "learning_rate": 1.2423146473779386e-05,
      "loss": 0.6392055511474609,
      "step": 420
    },
    {
      "epoch": 0.7775768535262206,
      "grad_norm": 2.504314661026001,
      "learning_rate": 1.224231464737794e-05,
      "loss": 0.5826800346374512,
      "step": 430
    },
    {
      "epoch": 0.7956600361663653,
      "grad_norm": 4.175527572631836,
      "learning_rate": 1.2061482820976493e-05,
      "loss": 0.7091936111450196,
      "step": 440
    },
    {
      "epoch": 0.8137432188065099,
      "grad_norm": 3.491664171218872,
      "learning_rate": 1.1880650994575048e-05,
      "loss": 0.6349629878997802,
      "step": 450
    },
    {
      "epoch": 0.8318264014466547,
      "grad_norm": 2.798557996749878,
      "learning_rate": 1.1699819168173599e-05,
      "loss": 0.6395931720733643,
      "step": 460
    },
    {
      "epoch": 0.8499095840867993,
      "grad_norm": 2.9762542247772217,
      "learning_rate": 1.1518987341772152e-05,
      "loss": 0.6638393402099609,
      "step": 470
    },
    {
      "epoch": 0.8679927667269439,
      "grad_norm": 6.243873119354248,
      "learning_rate": 1.1338155515370706e-05,
      "loss": 0.644569206237793,
      "step": 480
    },
    {
      "epoch": 0.8860759493670886,
      "grad_norm": 2.249807596206665,
      "learning_rate": 1.115732368896926e-05,
      "loss": 0.6678397178649902,
      "step": 490
    },
    {
      "epoch": 0.9041591320072333,
      "grad_norm": 2.9070706367492676,
      "learning_rate": 1.0976491862567812e-05,
      "loss": 0.6178314685821533,
      "step": 500
    },
    {
      "epoch": 0.9222423146473779,
      "grad_norm": 2.718322277069092,
      "learning_rate": 1.0795660036166367e-05,
      "loss": 0.7390040397644043,
      "step": 510
    },
    {
      "epoch": 0.9403254972875226,
      "grad_norm": 3.0838351249694824,
      "learning_rate": 1.061482820976492e-05,
      "loss": 0.6753194808959961,
      "step": 520
    },
    {
      "epoch": 0.9584086799276673,
      "grad_norm": 3.509866237640381,
      "learning_rate": 1.0433996383363474e-05,
      "loss": 0.64618821144104,
      "step": 530
    },
    {
      "epoch": 0.976491862567812,
      "grad_norm": 5.329741954803467,
      "learning_rate": 1.0253164556962025e-05,
      "loss": 0.6454991340637207,
      "step": 540
    },
    {
      "epoch": 0.9945750452079566,
      "grad_norm": 3.9165074825286865,
      "learning_rate": 1.0072332730560578e-05,
      "loss": 0.6682183265686035,
      "step": 550
    },
    {
      "epoch": 1.0126582278481013,
      "grad_norm": 6.488151550292969,
      "learning_rate": 9.891500904159133e-06,
      "loss": 0.6968020439147949,
      "step": 560
    },
    {
      "epoch": 1.030741410488246,
      "grad_norm": 2.8726303577423096,
      "learning_rate": 9.710669077757686e-06,
      "loss": 0.6044520378112793,
      "step": 570
    },
    {
      "epoch": 1.0488245931283906,
      "grad_norm": 3.1863815784454346,
      "learning_rate": 9.52983725135624e-06,
      "loss": 0.6639548778533936,
      "step": 580
    },
    {
      "epoch": 1.0669077757685352,
      "grad_norm": 6.235774993896484,
      "learning_rate": 9.349005424954792e-06,
      "loss": 0.6702439308166503,
      "step": 590
    },
    {
      "epoch": 1.0849909584086799,
      "grad_norm": 2.7426774501800537,
      "learning_rate": 9.168173598553346e-06,
      "loss": 0.6966180801391602,
      "step": 600
    },
    {
      "epoch": 1.1030741410488245,
      "grad_norm": 3.329580068588257,
      "learning_rate": 8.987341772151899e-06,
      "loss": 0.6508711338043213,
      "step": 610
    },
    {
      "epoch": 1.1211573236889691,
      "grad_norm": 3.1412034034729004,
      "learning_rate": 8.806509945750454e-06,
      "loss": 0.667992639541626,
      "step": 620
    },
    {
      "epoch": 1.139240506329114,
      "grad_norm": 2.885969877243042,
      "learning_rate": 8.625678119349007e-06,
      "loss": 0.6210862159729004,
      "step": 630
    },
    {
      "epoch": 1.1573236889692586,
      "grad_norm": 3.2200703620910645,
      "learning_rate": 8.44484629294756e-06,
      "loss": 0.695658016204834,
      "step": 640
    },
    {
      "epoch": 1.1754068716094033,
      "grad_norm": 6.30928897857666,
      "learning_rate": 8.264014466546112e-06,
      "loss": 0.6334959030151367,
      "step": 650
    },
    {
      "epoch": 1.193490054249548,
      "grad_norm": 3.040287971496582,
      "learning_rate": 8.083182640144665e-06,
      "loss": 0.6227005958557129,
      "step": 660
    },
    {
      "epoch": 1.2115732368896925,
      "grad_norm": 2.8500447273254395,
      "learning_rate": 7.90235081374322e-06,
      "loss": 0.6240445613861084,
      "step": 670
    },
    {
      "epoch": 1.2296564195298372,
      "grad_norm": 5.832006931304932,
      "learning_rate": 7.721518987341773e-06,
      "loss": 0.6514779090881347,
      "step": 680
    },
    {
      "epoch": 1.247739602169982,
      "grad_norm": 4.938108921051025,
      "learning_rate": 7.5406871609403265e-06,
      "loss": 0.5751829147338867,
      "step": 690
    },
    {
      "epoch": 1.2658227848101267,
      "grad_norm": 5.145878791809082,
      "learning_rate": 7.359855334538879e-06,
      "loss": 0.5940271854400635,
      "step": 700
    },
    {
      "epoch": 1.2839059674502713,
      "grad_norm": 2.4094138145446777,
      "learning_rate": 7.179023508137432e-06,
      "loss": 0.657178544998169,
      "step": 710
    },
    {
      "epoch": 1.301989150090416,
      "grad_norm": 5.391496658325195,
      "learning_rate": 6.998191681735986e-06,
      "loss": 0.673400592803955,
      "step": 720
    },
    {
      "epoch": 1.3200723327305606,
      "grad_norm": 5.44322395324707,
      "learning_rate": 6.81735985533454e-06,
      "loss": 0.5982741355895996,
      "step": 730
    },
    {
      "epoch": 1.3381555153707052,
      "grad_norm": 2.7836544513702393,
      "learning_rate": 6.636528028933093e-06,
      "loss": 0.7492713928222656,
      "step": 740
    },
    {
      "epoch": 1.3562386980108498,
      "grad_norm": 3.3611271381378174,
      "learning_rate": 6.4556962025316464e-06,
      "loss": 0.6090754508972168,
      "step": 750
    },
    {
      "epoch": 1.3743218806509945,
      "grad_norm": 3.8526313304901123,
      "learning_rate": 6.274864376130199e-06,
      "loss": 0.6573276519775391,
      "step": 760
    },
    {
      "epoch": 1.3924050632911391,
      "grad_norm": 3.5975260734558105,
      "learning_rate": 6.094032549728752e-06,
      "loss": 0.6694434642791748,
      "step": 770
    },
    {
      "epoch": 1.410488245931284,
      "grad_norm": 6.449615478515625,
      "learning_rate": 5.913200723327306e-06,
      "loss": 0.6733840942382813,
      "step": 780
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 6.901642322540283,
      "learning_rate": 5.73236889692586e-06,
      "loss": 0.6562162876129151,
      "step": 790
    },
    {
      "epoch": 1.4466546112115732,
      "grad_norm": 3.119332790374756,
      "learning_rate": 5.5515370705244135e-06,
      "loss": 0.6821319103240967,
      "step": 800
    },
    {
      "epoch": 1.4647377938517179,
      "grad_norm": 4.70084285736084,
      "learning_rate": 5.3707052441229655e-06,
      "loss": 0.6361209392547608,
      "step": 810
    },
    {
      "epoch": 1.4828209764918625,
      "grad_norm": 6.7865471839904785,
      "learning_rate": 5.189873417721519e-06,
      "loss": 0.6159725666046143,
      "step": 820
    },
    {
      "epoch": 1.5009041591320074,
      "grad_norm": 3.5240557193756104,
      "learning_rate": 5.009041591320073e-06,
      "loss": 0.6746084213256835,
      "step": 830
    },
    {
      "epoch": 1.518987341772152,
      "grad_norm": 2.951719284057617,
      "learning_rate": 4.828209764918626e-06,
      "loss": 0.6207268714904786,
      "step": 840
    },
    {
      "epoch": 1.5370705244122966,
      "grad_norm": 10.603142738342285,
      "learning_rate": 4.64737793851718e-06,
      "loss": 0.605379295349121,
      "step": 850
    },
    {
      "epoch": 1.5551537070524413,
      "grad_norm": 5.788105487823486,
      "learning_rate": 4.4665461121157326e-06,
      "loss": 0.6543875694274902,
      "step": 860
    },
    {
      "epoch": 1.573236889692586,
      "grad_norm": 4.379678249359131,
      "learning_rate": 4.2857142857142855e-06,
      "loss": 0.6283945083618164,
      "step": 870
    },
    {
      "epoch": 1.5913200723327305,
      "grad_norm": 5.924643516540527,
      "learning_rate": 4.104882459312839e-06,
      "loss": 0.706660795211792,
      "step": 880
    },
    {
      "epoch": 1.6094032549728752,
      "grad_norm": 4.141601085662842,
      "learning_rate": 3.924050632911393e-06,
      "loss": 0.609566068649292,
      "step": 890
    },
    {
      "epoch": 1.6274864376130198,
      "grad_norm": 3.5074007511138916,
      "learning_rate": 3.7432188065099463e-06,
      "loss": 0.6924904346466064,
      "step": 900
    },
    {
      "epoch": 1.6455696202531644,
      "grad_norm": 3.799738883972168,
      "learning_rate": 3.562386980108499e-06,
      "loss": 0.6358031749725341,
      "step": 910
    },
    {
      "epoch": 1.663652802893309,
      "grad_norm": 8.582143783569336,
      "learning_rate": 3.381555153707053e-06,
      "loss": 0.6596198558807373,
      "step": 920
    },
    {
      "epoch": 1.681735985533454,
      "grad_norm": 3.3267476558685303,
      "learning_rate": 3.200723327305606e-06,
      "loss": 0.6133617877960205,
      "step": 930
    },
    {
      "epoch": 1.6998191681735986,
      "grad_norm": 6.502720355987549,
      "learning_rate": 3.0198915009041596e-06,
      "loss": 0.6129802227020263,
      "step": 940
    },
    {
      "epoch": 1.7179023508137432,
      "grad_norm": 3.3111984729766846,
      "learning_rate": 2.839059674502713e-06,
      "loss": 0.5881713390350342,
      "step": 950
    },
    {
      "epoch": 1.7359855334538878,
      "grad_norm": 3.1585092544555664,
      "learning_rate": 2.6582278481012658e-06,
      "loss": 0.6688329696655273,
      "step": 960
    },
    {
      "epoch": 1.7540687160940327,
      "grad_norm": 4.745516777038574,
      "learning_rate": 2.4773960216998195e-06,
      "loss": 0.5972410678863526,
      "step": 970
    },
    {
      "epoch": 1.7721518987341773,
      "grad_norm": 7.272952556610107,
      "learning_rate": 2.296564195298373e-06,
      "loss": 0.6388853549957275,
      "step": 980
    },
    {
      "epoch": 1.790235081374322,
      "grad_norm": 3.493927240371704,
      "learning_rate": 2.115732368896926e-06,
      "loss": 0.6534886837005616,
      "step": 990
    },
    {
      "epoch": 1.8083182640144666,
      "grad_norm": 8.220490455627441,
      "learning_rate": 1.9349005424954795e-06,
      "loss": 0.6561303615570069,
      "step": 1000
    },
    {
      "epoch": 1.8264014466546112,
      "grad_norm": 5.665515899658203,
      "learning_rate": 1.7540687160940326e-06,
      "loss": 0.6610420703887939,
      "step": 1010
    },
    {
      "epoch": 1.8444846292947559,
      "grad_norm": 2.9892630577087402,
      "learning_rate": 1.573236889692586e-06,
      "loss": 0.6655667304992676,
      "step": 1020
    },
    {
      "epoch": 1.8625678119349005,
      "grad_norm": 3.6903631687164307,
      "learning_rate": 1.3924050632911392e-06,
      "loss": 0.6356136322021484,
      "step": 1030
    },
    {
      "epoch": 1.8806509945750451,
      "grad_norm": 5.30231237411499,
      "learning_rate": 1.2115732368896926e-06,
      "loss": 0.6310811519622803,
      "step": 1040
    },
    {
      "epoch": 1.8987341772151898,
      "grad_norm": 3.4188971519470215,
      "learning_rate": 1.030741410488246e-06,
      "loss": 0.6885749340057373,
      "step": 1050
    },
    {
      "epoch": 1.9168173598553344,
      "grad_norm": 3.628941059112549,
      "learning_rate": 8.499095840867993e-07,
      "loss": 0.6515541553497315,
      "step": 1060
    },
    {
      "epoch": 1.934900542495479,
      "grad_norm": 3.2989680767059326,
      "learning_rate": 6.690777576853528e-07,
      "loss": 0.6242845535278321,
      "step": 1070
    },
    {
      "epoch": 1.952983725135624,
      "grad_norm": 2.838397741317749,
      "learning_rate": 4.88245931283906e-07,
      "loss": 0.5940973281860351,
      "step": 1080
    },
    {
      "epoch": 1.9710669077757685,
      "grad_norm": 3.621577739715576,
      "learning_rate": 3.0741410488245934e-07,
      "loss": 0.59226393699646,
      "step": 1090
    },
    {
      "epoch": 1.9891500904159132,
      "grad_norm": 6.472443580627441,
      "learning_rate": 1.2658227848101266e-07,
      "loss": 0.6239461421966552,
      "step": 1100
    }
  ],
  "logging_steps": 10,
  "max_steps": 1106,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1403888179200.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
